{"cells":[{"cell_type":"markdown","source":["## Import and dataloaders\n","\n","Before starting this tutorial, let's  import and define our dataloaders for CIFAR10."],"metadata":{"id":"Z1P5KUInW6uY"}},{"cell_type":"code","source":["# Import PyTorch\n","import torch\n","from torch import nn\n","\n","# Import torchvision\n","import torchvision\n","from torchvision import datasets\n","# from torchvision.transforms import ToTensor\n","import torchvision.transforms as transforms\n","\n","# Import matplotlib for visualization\n","import matplotlib.pyplot as plt\n","\n","# Check versions\n","# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\n","print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KuPXfbU0XFIw","executionInfo":{"status":"ok","timestamp":1694488158106,"user_tz":-480,"elapsed":7417,"user":{"displayName":"Hasan Firdaus","userId":"10430079172461636773"}},"outputId":"6a8eb487-e80e-4ae2-8d42-86741ab88602"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version: 2.0.1+cu118\n","torchvision version: 0.15.2+cu118\n"]}]},{"cell_type":"code","source":["# Define transformations\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n","\n","# Setup training data\n","train_data = datasets.CIFAR10(\n","    root=\"data\", # where to download data to?\n","    train=True, # get training data\n","    download=True, # download data if it doesn't exist on disk\n","    transform=transform, # images come as PIL format, we want to turn into Torch tensors and normalize\n","    target_transform=None # you can transform labels as well\n",")\n","\n","# Setup testing data\n","test_data = datasets.CIFAR10(\n","    root=\"data\",\n","    train=False, # get test data\n","    download=True,\n","    transform=transform\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Nq55v6GXSqi","executionInfo":{"status":"ok","timestamp":1694488171213,"user_tz":-480,"elapsed":7248,"user":{"displayName":"Hasan Firdaus","userId":"10430079172461636773"}},"outputId":"512b257e-d7e1-49b9-cad8-737ebbb0ceb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:02<00:00, 79324401.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/cifar-10-python.tar.gz to data\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","# Setup the batch size hyperparameter\n","BATCH_SIZE = 4\n","\n","# Turn datasets into iterables (batches)\n","train_dataloader = DataLoader(train_data, # dataset to turn into iterable\n","    batch_size=BATCH_SIZE, # how many samples per batch?\n","    shuffle=True # shuffle data every epoch?\n",")\n","\n","test_dataloader = DataLoader(test_data,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False # don't necessarily have to shuffle the testing data\n",")\n","\n","# Let's check out what we've created\n","print(f\"Dataloaders: {train_dataloader, test_dataloader}\")\n","print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n","print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OANDTmKwXYww","executionInfo":{"status":"ok","timestamp":1694488178720,"user_tz":-480,"elapsed":332,"user":{"displayName":"Hasan Firdaus","userId":"10430079172461636773"}},"outputId":"be031384-1052-4a0b-afee-6723f1c7baff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x798eb0ac7a90>, <torch.utils.data.dataloader.DataLoader object at 0x798eb0ac7f10>)\n","Length of train dataloader: 12500 batches of 4\n","Length of test dataloader: 2500 batches of 4\n"]}]},{"cell_type":"markdown","metadata":{"id":"HVihl7rPFO9W"},"source":["## Batch Normalization and Dropout\n","\n","Discover how batch normalization and dropout improve a model's accuracy.\n","\n","We will be covering:\n","\n","- Batch Normalization\n","\n","- Notations\n","\n","- Advantages and disadvantages of using batch normalization\n","\n","- Dropout\n","\n","### Batch Normalization\n","\n","If you open any introductory machine learning textbook, you will find the idea of **input scaling**. It is undesirable to train a model with **gradient descent** with non-normalized input features.\n","\n","Let’s start with an intuitive example to understand why we want normalization inside any model.\n","\n","Suppose you have an input feature $x1$ in the range [0,10000] and another feature $x2$ in the range [0,1]. Any linear combination would ignore $x2$ such that $x1*w1 + x2*w2 \\approx x1$, since our weights are initialized in a very tiny range like [-1,1].\n","\n","We encounter the same issues inside the layers of deep neural networks. In this lesson, we will propagate this idea inside the NN.\n","\n","> If we think out of the box, any intermediate layer is conceptually the same as the input layer; it accepts features and transforms them"]},{"cell_type":"markdown","metadata":{"id":"78X-xhlyFO9y"},"source":["### Notations\n","\n","Throughout this lesson, $N$ will be the batch size, $H$ will refer to the height, $W$ to the width, and $C$ to the feature channels. The greek letter $\\mu()$ refers to mean and the greek letter $\\sigma()$ refers to standard deviation.\n","\n","The batch features are denoted by $x$ with a shape of $[N, C, H, W]$.\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig22.PNG)\n","\n","We will visualize the 4D activation maps x by **merging the spatial dimensions**. Now, we have a 3D shape that looks like this:\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig23.PNG)\n","\n","The most dominant solution is batch normalization. Let’s see how it works.\n","\n","> Batch Normalization (BN) normalizes the mean and standard deviation **for each individual feature channel/map**.\n","\n","First of all, the mean and standard deviation are first-order statistics, and thus they relate to the **global characteristics** (such as the image style).\n","\n","In this way, we somehow blend the global characteristics. Such a strategy is effective when we want our representation to share these characteristics. This is the reason that we widely utilize BN in downstream tasks (i.e., image classification).\n","\n","From a mathematical point of view, **you can think of it as bringing the features of the image in the same range**.\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig24.PNG)\n","\n","Specifically, we demand from our features to follow a Gaussian distribution with zero mean and unit variance. Mathematically, this can be expressed as:\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig25.PNG)\n","\n","The index $c$ denotes the per-channel (feature map) mean.\n","\n","Let’s see this operation visually:\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig26.PNG)\n","\n","Notably, the spatial dimensions as well as the image batch are averaged. This way, **we concentrate our features in a compact Gaussian-like space**, which is usually beneficial.\n","\n","In fact, $\\gamma$ and $\\beta$ correspond to the trainable parameters that result in the linear/affine transformation, which is different for all channels.\n","\n","Specifically $\\gamma$ and $\\beta$ are vectors with the channel dimensionality."]},{"cell_type":"markdown","metadata":{"id":"Iq4OEKm3FO95"},"source":["### Advantages and disadvantages of using batch normalization\n","\n","The following are some **advantages** of BN:\n","\n","- BN accelerates the training of deep neural networks and tackles the vanishing gradient problem.\n","\n","- For every input mini-batch, we calculate different statistics. This introduces some sort of regularization. Regularization refers to any form of technique/constraint that restricts the complexity of a deep neural network during training.\n","\n","- BN also has a beneficial effect on the gradient flow through the network. It reduces the dependence of gradients on the scale of the parameters or of their initial values. This allows us to use much higher learning rates.\n","\n","- In theory, BN makes it possible to use saturating nonlinearities by preventing the network from getting stuck, but we just use nn.ReLU().\n","\n","- BN makes the gradients more predictive.\n","\n","BN has the following **disadvantages**:\n","\n","- Batch normalization may cause inaccurate estimation of batch statistics when we have a small batch size. This increases the model error. In tasks such as image segmentation, the batch size is usually too small. BN needs a sufficiently large batch size.\n","\n","Let’s now implement batch normalization from scratch for images of size $[N, C, H, W]$. All you have to do is transform the above equations to Pytorch. The tricky part is to correctly figure out the sizes of each tensor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c9RMg8gtFO9-"},"outputs":[],"source":["import torch\n","\n","# Gamma and beta are provided as 1d tensors.\n","# X is the data in a mini-batch\n","\n","def batchnorm(X, gamma, beta):\n","\n","    # extract the dimensions\n","    N, C, H, W = list(X.size())\n","    # mini-batch mean\n","    mean = torch.mean(X, dim=(0, 2, 3))\n","    # mini-batch variance\n","    variance = torch.mean((X - mean.reshape((1, C, 1, 1))) ** 2, dim=(0, 2, 3))\n","    # normalize\n","    X_hat = (X - mean.reshape((1, C, 1, 1))) * 1.0 / torch.sqrt(variance.reshape((1, C, 1, 1)) )\n","    # scale and shift\n","    out = gamma.reshape((1, C, 1, 1)) * X_hat + beta.reshape((1, C, 1, 1))\n","\n","    return out"]},{"cell_type":"code","source":["# get some random training images\n","dataiter = iter(train_dataloader)\n","images, labels = next(dataiter)\n","print(images)\n","\n","BN_layer = nn.BatchNorm2d(3)\n","output_BN = BN_layer(images)\n","print(output_BN)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xiiu6gbyXiV5","executionInfo":{"status":"ok","timestamp":1694488768081,"user_tz":-480,"elapsed":4,"user":{"displayName":"Hasan Firdaus","userId":"10430079172461636773"}},"outputId":"22af60d5-553e-4557-e564-a07d66496641"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 3, 32, 32])\n","torch.Size([4, 3, 32, 32])\n"]}]},{"cell_type":"markdown","metadata":{"id":"OKa3BdDeFO-F"},"source":["### Dropout\n","\n","Another technique to train deep learning models is dropout.\n","\n","Conceptually, dropout approximates training a large number of neural networks with different architectures in parallel.\n","\n","> In practice, during training, some number of layer outputs are randomly ignored (dropped out) with probability $p$.\n","\n","Thus, the same layer will alter its connectivity and will search for alternative paths to convey the information in the next layer. As a result, each update to a layer during training is performed with a different “view” of the configured layer.\n","\n","“Dropping” values means temporarily removing them from the network for the current forward pass along with all its incoming and outgoing connections.\n","\n","Dropout has the effect of making the training process noisy. The choice of the probability $p$ depends on the architecture.\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig27.PNG)\n","This conceptualization suggests that perhaps dropout breaks-up situations where network layers co-adapt to correct mistakes from prior layers, in turn making the model more robust.\n","\n","The neural network will adapt in a way that prevents overfitting, which refers to poor generalization to unseen data.\n","\n","Dropout increases the sparsity of the network and in general encourages sparse representations!\n","\n","You can find an example in the code below .\n","\n","Notice that each value will be zeroed with a probability of p=0.5. Nonetheless, that doesn’t imply that the output will be 50% zeroed-out every time."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TkEyw3pDFO-I","executionInfo":{"status":"ok","timestamp":1694488925892,"user_tz":-480,"elapsed":336,"user":{"displayName":"Hasan Firdaus","userId":"10430079172461636773"}},"outputId":"683c061a-6ccb-4e68-b798-7dbf71507b19"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.4499, 0.6612, 0.8199, 0.8643, 0.8594, 0.6504, 0.5860, 0.3649]])\n","tensor([[0.0000, 0.0000, 1.6398, 0.0000, 0.0000, 1.3008, 1.1721, 0.0000]])\n","tensor([[0.8997, 1.3223, 0.0000, 0.0000, 1.7188, 1.3008, 1.1721, 0.0000]])\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","inp = torch.rand(1,8)\n","dropout_layer = nn.Dropout(0.5)\n","out1 = dropout_layer(inp)\n","out2 = dropout_layer(inp)\n","print(inp)\n","print(out1)\n","print(out2)"]},{"cell_type":"markdown","metadata":{"id":"G2vah6ZTFO-L"},"source":["### Training with batchnorm\n","\n","Now, it's the time to apply batchnorm and dropout! Let's copy and paste the code for training a CNN in our previous notebook."]},{"cell_type":"markdown","metadata":{"id":"PhztnMavFO-X"},"source":["Here, try to incorporate batchnorm as a layer in this vanilla CNN.\n","\n","Note that in pytorch, batchnorm can be implemented using `nn.BatchNorm2d(num_features)` for 2D input and `nn.BatchNorm2d(num_features)` for 1D input."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aocAQZf_FO-Z"},"outputs":[],"source":["#1. DEFINE THE CNN WITH BATCHNORM\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.batchnorm1 = nn.BatchNorm2d(6) # defining our batchnorm1 layer\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.batchnorm2 = nn.BatchNorm2d(16), # defining our batchnorm2 layer\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","        self.relu = nn.ReLU()\n","\n","\n","    def forward(self, x):\n","        x = self.pool(self.relu(self.conv1(x)))\n","        # INCLUDE BATCHNORM IN THE FORWARD METHOD #\n","        x = self.batchnorm1(x)\n","        x = self.pool(self.relu(self.conv2(x)))\n","        # INCLUDE BATCHNORM IN THE FORWARD METHOD #\n","        x = self.batchnorm2(x)\n","        x = x.view(-1, 16 * 5 * 5)\n","        x = self.relu(self.fc1(x))\n","        # INCLUDE DROPOUT IN THE FORWARD METHOD #\n","        x = self.dropout(x)\n","        x = self.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vgWT1BGZFO-b","executionInfo":{"status":"ok","timestamp":1694489555290,"user_tz":-480,"elapsed":352,"user":{"displayName":"Hasan Firdaus","userId":"10430079172461636773"}},"outputId":"1c0a4470-8c17-4144-b208-c1317e7b25d8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CNN(\n","  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (batchnorm1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=400, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=84, bias=True)\n","  (fc3): Linear(in_features=84, out_features=10, bias=True)\n","  (relu): ReLU()\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")"]},"metadata":{},"execution_count":16}],"source":["model = CNN() # need to instantiate the network to be used in instance method\n","\n","# LOSS AND OPTIMIZER\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","# move the model to GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iqUb-WREFO-e"},"outputs":[],"source":["import time\n","from tqdm.auto import tqdm\n","\n","def train_and_validate(model, loss_criterion, optimizer, train_dataloader, test_dataloader, epochs=25, device='cuda'):\n","    '''\n","    Function to train and validate\n","    Parameters\n","        :param model: Model to train and validate\n","        :param loss_criterion: Loss Criterion to minimize\n","        :param optimizer: Optimizer for computing gradients\n","        :param train_dataloader: DataLoader for training data\n","        :param test_dataloader: DataLoader for test/validation data\n","        :param epochs: Number of epochs (default=25)\n","        :param device: Device to perform computations ('cuda' or 'cpu')\n","\n","    Returns\n","        model: Trained Model with best validation accuracy\n","        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n","    '''\n","\n","    start = time.time()\n","    history = []\n","    best_acc = 0.0\n","\n","    # accuracy = torchmetrics.Accuracy(device=device)\n","    # Initialize the accuracy metric from torchmetrics\n","    # accuracy = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n","\n","    for epoch in tqdm(range(epochs)):\n","        epoch_start = time.time()\n","        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n","\n","        model.train()\n","\n","        train_loss = 0.0\n","        train_acc = 0.0\n","\n","        valid_loss = 0.0\n","        valid_acc = 0.0\n","\n","        for i, (inputs, labels) in enumerate(train_dataloader):\n","\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            # Clean existing gradients\n","            optimizer.zero_grad()\n","\n","            # Forward pass - compute outputs on input data using the model\n","            outputs = model(inputs)\n","\n","            # Compute loss\n","            loss = loss_criterion(outputs, labels)\n","\n","            # Backpropagate the gradients\n","            loss.backward()\n","\n","            # Update the parameters\n","            optimizer.step()\n","\n","            # Compute the total loss for the batch and add it to train_loss\n","            train_loss += loss.item() * inputs.size(0)\n","\n","            # Compute the accuracy\n","            ret, predictions = torch.max(outputs.data, 1)\n","            correct_counts = predictions.eq(labels.data.view_as(predictions))\n","\n","            # Convert correct_counts to float and then compute the mean\n","            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","\n","            # Compute total accuracy in the whole batch and add to train_acc\n","            train_acc += acc.item() * inputs.size(0)\n","\n","        # Validation - No gradient tracking needed\n","        with torch.no_grad():\n","\n","            model.eval()\n","\n","            # Validation loop\n","            for j, (inputs, labels) in enumerate(test_dataloader):\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # Forward pass - compute outputs on input data using the model\n","                outputs = model(inputs)\n","\n","                # Compute loss\n","                loss = loss_criterion(outputs, labels)\n","\n","                # Compute the total loss for the batch and add it to valid_loss\n","                valid_loss += loss.item() * inputs.size(0)\n","\n","                # Calculate validation accuracy\n","                ret, predictions = torch.max(outputs.data, 1)\n","                correct_counts = predictions.eq(labels.data.view_as(predictions))\n","\n","                # Convert correct_counts to float and then compute the mean\n","                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","\n","                # Compute total accuracy in the whole batch and add to valid_acc\n","                valid_acc += acc.item() * inputs.size(0)\n","\n","\n","        # Find average training loss and training accuracy\n","        avg_train_loss = train_loss / len(train_dataloader.dataset)\n","        avg_train_acc = train_acc / len(train_dataloader.dataset)\n","\n","        # Find average validation loss and training accuracy\n","        avg_test_loss = valid_loss / len(test_dataloader.dataset)\n","        avg_test_acc = valid_acc / len(test_dataloader.dataset)\n","\n","        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n","\n","        epoch_end = time.time()\n","\n","        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc * 100, avg_test_loss, avg_test_acc * 100, epoch_end - epoch_start))\n","\n","        # Save if the model has best accuracy till now\n","        if avg_test_acc > best_acc:\n","            best_acc = avg_test_acc\n","            best_model = model\n","            torch.save(best_model, 'best_model.pt')\n","\n","    return best_model, history"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":420,"referenced_widgets":["b8c60b03b8da4a86a6e2432f3ccf083f","20e3275a04544fb8a798c586118cd60e","9c7269e768b84edda3c596693b42dfde","d76d63cb12e34beea20bfe16c01cd303","e7cb32ca0fca40ecb09edbf91c5b7ffb","31b42fad3e24454cacf4604cf7033790","bd4764d691714fdea8775a94a53d59bc","1791d8650e3f4cdea7867568b9be64ba","18efb313784143bfb778842303a67ac7","d17007b6c7614b42964241a7d1789f3b","7c055ead6d2f4380bb65492dfd2ac97e"]},"id":"ngaHvvWZFO-i","outputId":"5c719c3a-4895-45ec-fed2-9b0ccda0e2ab","executionInfo":{"status":"error","timestamp":1694489617725,"user_tz":-480,"elapsed":362,"user":{"displayName":"Hasan Firdaus","userId":"10430079172461636773"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8c60b03b8da4a86a6e2432f3ccf083f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 1/10\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-83a0d5295cea>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-19-5da6e512da82>\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, loss_criterion, optimizer, train_dataloader, test_dataloader, epochs, device)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LAZY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"]}],"source":["# 4. Train the model for 10 epochs\n","\n","num_epochs = 10\n","trained_model, history = train_and_validate(model, loss_fn, optimizer, train_dataloader, test_dataloader, num_epochs)"]},{"cell_type":"code","source":["plot_loss(history)\n","plot_accuracy(history)\n","plot_confusionMatrix(trained_model, test_dataloader)"],"metadata":{"id":"wN477QQ4e6h-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DLDK3OpMFO-k"},"source":["### Training with dropout\n","\n","Adding dropout to your PyTorch models is very straightforward with the `torch.nn.Dropout` class, which takes in the dropout rate – the probability of a neuron being deactivated – as a parameter.\n","\n","Go back to the CNN class and try to incorporate dropout as an additional layer in the fully connected layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ksfu_3QoFO-m"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.7 (pytorch_hasan)","language":"python","name":"torch"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"provenance":[],"gpuType":"T4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b8c60b03b8da4a86a6e2432f3ccf083f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_20e3275a04544fb8a798c586118cd60e","IPY_MODEL_9c7269e768b84edda3c596693b42dfde","IPY_MODEL_d76d63cb12e34beea20bfe16c01cd303"],"layout":"IPY_MODEL_e7cb32ca0fca40ecb09edbf91c5b7ffb"}},"20e3275a04544fb8a798c586118cd60e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31b42fad3e24454cacf4604cf7033790","placeholder":"​","style":"IPY_MODEL_bd4764d691714fdea8775a94a53d59bc","value":"  0%"}},"9c7269e768b84edda3c596693b42dfde":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_1791d8650e3f4cdea7867568b9be64ba","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_18efb313784143bfb778842303a67ac7","value":0}},"d76d63cb12e34beea20bfe16c01cd303":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d17007b6c7614b42964241a7d1789f3b","placeholder":"​","style":"IPY_MODEL_7c055ead6d2f4380bb65492dfd2ac97e","value":" 0/10 [00:00&lt;?, ?it/s]"}},"e7cb32ca0fca40ecb09edbf91c5b7ffb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31b42fad3e24454cacf4604cf7033790":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd4764d691714fdea8775a94a53d59bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1791d8650e3f4cdea7867568b9be64ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18efb313784143bfb778842303a67ac7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d17007b6c7614b42964241a7d1789f3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c055ead6d2f4380bb65492dfd2ac97e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}